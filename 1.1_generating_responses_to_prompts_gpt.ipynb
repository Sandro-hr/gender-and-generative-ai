{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d285f285",
   "metadata": {},
   "source": [
    "## Import Libraries & Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c62013c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8f6642-0ff7-448e-b186-d0794ce17d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove column width to ensure that all characters are displayed\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7a2269-6db8-4372-b75e-720646338398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add scripts and data path to the list of search paths\n",
    "script_dir = Path(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "sys.path.append(str(script_dir / \".\" / \"src\" / \"scripts\"))\n",
    "sys.path.append(str(script_dir / \".\" / \"data\" / \"products\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "934177ad-cf73-4ab8-8b3c-d6eac616a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import code to automate querying of GPT\n",
    "from gpt import QueryGPT\n",
    "\n",
    "# import list of products\n",
    "from products import products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06fa3009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from the .env file\n",
    "# The .env file is where the \"OPEN_AI_API_KEY\" is stored\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c975f3a1-e313-4d31-8a47-35665223c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Open AI Key\n",
    "open_ai_api_key = os.environ['OPEN_AI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f6b61",
   "metadata": {},
   "source": [
    "## Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "249211af-263b-4e42-9d5a-6e9f0b2abe1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beer',\n",
       " 'chocolate',\n",
       " 'ice cream',\n",
       " 'protein powder',\n",
       " 'a weight loss programme',\n",
       " 'a lawnmower',\n",
       " 'a car',\n",
       " 'a diy store',\n",
       " 'a supermarket',\n",
       " 'a clothes shop',\n",
       " 'furniture polish',\n",
       " 'a washing machine',\n",
       " 'dishwasher tablets',\n",
       " 'a vacuum cleaner',\n",
       " 'candles',\n",
       " 'bubble bath',\n",
       " 'curtains',\n",
       " 'electric drills',\n",
       " 'nappies',\n",
       " 'a science museum',\n",
       " 'an art gallery',\n",
       " 'a bookshop',\n",
       " 'a games console',\n",
       " 'a social network',\n",
       " 'a yoga class',\n",
       " 'a weightlifting class',\n",
       " 'a golf club',\n",
       " 'therapy']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview list of products \n",
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23326c-0c77-4126-bd9f-f2ba73dead9e",
   "metadata": {},
   "source": [
    "## Generate Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5606fc5f-81c0-45e7-bd96-4efa9390f77d",
   "metadata": {},
   "source": [
    "### GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e24e7119-b8a3-4962-b095-3f21668faf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifiy model\n",
    "model = 'gpt-3.5-turbo'\n",
    "\n",
    "# initiate query object\n",
    "query_object = QueryGPT(open_ai_api_key=open_ai_api_key, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef708078",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/sandro.rodriguez/Desktop/gender-and-generative-ai/logs/20240703141003_chatgpt.log'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m responses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m search_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a script for an advert promoting beer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_string\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msearch_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m responses\u001b[38;5;241m.\u001b[39mappend(response)\n",
      "File \u001b[0;32m~/Desktop/gender-and-generative-ai/src/scripts/gpt.py:18\u001b[0m, in \u001b[0;36mQueryGPT.query_gpt\u001b[0;34m(self, search_string)\u001b[0m\n\u001b[1;32m     15\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__open_ai_api_key\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Create log file\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mlogging\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasicConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogs/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_chatgpt.log\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEBUG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(asctime)s\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mChatCompletion\u001b[38;5;241m.\u001b[39mcreate(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__model, \n\u001b[1;32m     23\u001b[0m                                             messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:search_string}], temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/logging/__init__.py:2003\u001b[0m, in \u001b[0;36mbasicConfig\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m   2001\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   2002\u001b[0m         errors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2003\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mFileHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2004\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2006\u001b[0m     stream \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/logging/__init__.py:1146\u001b[0m, in \u001b[0;36mFileHandler.__init__\u001b[0;34m(self, filename, mode, encoding, delay, errors)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1146\u001b[0m     StreamHandler\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/logging/__init__.py:1175\u001b[0m, in \u001b[0;36mFileHandler._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;124;03m    Open the current base file with the (original) mode and encoding.\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m    Return the resulting stream.\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbaseFilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/sandro.rodriguez/Desktop/gender-and-generative-ai/logs/20240703141003_chatgpt.log'"
     ]
    }
   ],
   "source": [
    "# create empty list to store responses to the prompt\n",
    "responses = []\n",
    "\n",
    "search_string = f\"Write a script for an advert promoting beer\"\n",
    "\n",
    "response = query_object.query_gpt(search_string = search_string)\n",
    "\n",
    "\n",
    "responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87a5844-c773-4707-be07-c8126c2b77ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list to store responses to the prompt\n",
    "responses = []\n",
    "\n",
    "# For each product, run prompt 40 times - generate a sufficiently large dataset\n",
    "for iteration in list(range(0,40)):\n",
    "    \n",
    "    for product in products:\n",
    "\n",
    "        # the search string specifies the prompt that is used\n",
    "        # query the Open AI API using hte prompt \"Write a script for an advert promoting X\"\n",
    "        search_string = f\"Write a script for an advert promoting {product}\"\n",
    "        \n",
    "        response = query_object.query_gpt(search_string = search_string)\n",
    "\n",
    "        # Append response to list\n",
    "        responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the number of times the products list is replicated with the number \n",
    "products_multiplied = []\n",
    "\n",
    "for i in list(range(0,40)):\n",
    "    products_multiplied = products_multiplied+products\n",
    "\n",
    "# This code needs to be updated with a new file name to ensure that previous responses are not overwritten\n",
    "\n",
    "# Create a dictionary with all relevant parts of the response\n",
    "list_of_responses = []\n",
    "\n",
    "for i, response in enumerate(responses):\n",
    "    \n",
    "    if isinstance(response, dict):\n",
    " \n",
    "        response_dict = {}\n",
    "        response_dict['unix_timestamp'] = response['created']\n",
    "        response_dict['id'] = response['id']\n",
    "        response_dict['prompt'] = f\"Write a script for an advert promoting {products_multiplied[i]}\"\n",
    "        response_dict['response'] = response['choices'][0]['message']['content']\n",
    "        response_dict['model'] = response['model']\n",
    "        response_dict['prompt_tokens'] = response['usage']['prompt_tokens']\n",
    "        response_dict['completion_tokens'] = response['usage']['completion_tokens']\n",
    "\n",
    "        list_of_responses.append(response_dict)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        continue\n",
    "\n",
    "# Convert dictionary to json\n",
    "response_json = json.dumps(list_of_responses)\n",
    "\n",
    "# Dump the json file \n",
    "# Update the file name so nothing is overwritten\n",
    "out_file = open(f\"\"\"data/raw_data/gpt3.5_responses_bulk_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.json\"\"\", \"w\")\n",
    "json.dump(response_json,out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81f83e6-58ff-48a7-b612-de504fd00721",
   "metadata": {},
   "source": [
    "### GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8eaa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifiy model\n",
    "model = 'gpt-4'\n",
    "\n",
    "# initiate query object\n",
    "query_object = QueryGPT(open_ai_api_key=open_ai_api_key, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f095569c-b883-4ee9-a3c2-363f5fe2be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list to store responses to the prompt\n",
    "responses = []\n",
    "\n",
    "# For each product, run prompt 40 times - generate a sufficiently large dataset\n",
    "for iteration in list(range(0,40)):\n",
    "    \n",
    "    for product in products:\n",
    "\n",
    "        # the search string specifies the prompt that is used\n",
    "        # query the Open AI API using hte prompt \"Write a script for an advert promoting X\"\n",
    "        search_string = f\"Write a script for an advert promoting {product}\"\n",
    "        \n",
    "        response = query_object.query_gpt(search_string = search_string)\n",
    "\n",
    "        # Append response to list\n",
    "        responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d43a5b1-24c4-4de7-9047-3c61efd45700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the number of times the products list is replicated with the number \n",
    "products_multiplied = []\n",
    "\n",
    "for i in list(range(0,40)):\n",
    "    products_multiplied = products_multiplied+products\n",
    "\n",
    "# This code needs to be updated with a new file name to ensure that previous responses are not overwritten\n",
    "\n",
    "# Create a dictionary with all relevant parts of the response\n",
    "list_of_responses = []\n",
    "\n",
    "for i, response in enumerate(responses):\n",
    "    \n",
    "    if isinstance(response, dict):\n",
    " \n",
    "        response_dict = {}\n",
    "        response_dict['unix_timestamp'] = response['created']\n",
    "        response_dict['id'] = response['id']\n",
    "        response_dict['prompt'] = f\"Write a script for an advert promoting {products_multiplied[i]}\"\n",
    "        response_dict['response'] = response['choices'][0]['message']['content']\n",
    "        response_dict['model'] = response['model']\n",
    "        response_dict['prompt_tokens'] = response['usage']['prompt_tokens']\n",
    "        response_dict['completion_tokens'] = response['usage']['completion_tokens']\n",
    "\n",
    "        list_of_responses.append(response_dict)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        continue\n",
    "\n",
    "# Convert dictionary to json\n",
    "response_json = json.dumps(list_of_responses)\n",
    "\n",
    "# Dump the json file \n",
    "# Update the file name so nothing is overwritten\n",
    "out_file = open(f\"\"\"data/raw_data/gpt4_responses_bulk_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.json\"\"\", \"w\")\n",
    "json.dump(response_json,out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2b994-1394-4b81-a653-2dc79eec1b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
